{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lf7Ciw1ILUJM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision import datasets\n",
        "from torch.optim import SGD\n",
        "from torch.optim import Adam\n",
        "\n",
        "import neptune\n",
        "import lovely_tensors as lt\n",
        "\n",
        "from neptune_creds import api_token\n",
        "\n",
        "\n",
        "lt.monkey_patch()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print( f\"device: {device}\" )\n",
        "\n",
        "\n",
        "# Umschalten zwischen Colab oder lokaler Installation\n",
        "USING_COLAB = False\n",
        "\n",
        "if USING_COLAB:\n",
        "  from google.colab import drive\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vQXCN16HLUJR"
      },
      "source": [
        "Download and load the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QRFb8iwnLUJU"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.MNIST('data/', download=True, train=True)\n",
        "train_images = train_set.data\n",
        "train_targets = train_set.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GnVk1LThLUJV"
      },
      "outputs": [],
      "source": [
        "test_set = datasets.MNIST('data/', download=True, train=False)\n",
        "test_images = test_set.data\n",
        "test_targets = test_set.targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8rolrYZBLUJW"
      },
      "outputs": [],
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        x = x.float()/255\n",
        "        x = x.unsqueeze(1)\n",
        "        x = x.view(-1,28*28)\n",
        "        self.x, self.y = x, y\n",
        "    def __getitem__(self, ix):\n",
        "        x, y = self.x[ix], self.y[ix]\n",
        "        return x.to(device), y.to(device)\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    # m.weight.data.fill_(1)\n",
        "    # m.weight.data.uniform_(-0.1, 0.1)\n",
        "    m.weight.data.normal_(0.0, 0.1)\n",
        "    if m.bias is not None:\n",
        "      m.bias.data.fill_(0)\n",
        "      \n",
        "      \n",
        "def train_batch(x, y, model, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  prediction = model(x)\n",
        "  batch_loss = loss_fn(prediction, y)\n",
        "  batch_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  return batch_loss.item()\n",
        "\n",
        "\n",
        "def accuracy(x, y, model):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = model(x)\n",
        "  max_values, argmaxes = prediction.max(-1)\n",
        "  is_correct = argmaxes == y\n",
        "  return is_correct.cpu().numpy().tolist()\n",
        "\n",
        "\n",
        "def loss(x, y, model, loss_fn):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = model(x)\n",
        "    loss = loss_fn(prediction, y)\n",
        "  return loss.item()\n",
        "\n",
        "\n",
        "def get_data(batch_size=32):\n",
        "    train = MNISTDataset(train_images, train_targets)\n",
        "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    test = MNISTDataset(test_images, test_targets)\n",
        "    test_dl = DataLoader(test, batch_size=len(test_images), shuffle=True)\n",
        "    return train_dl, test_dl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bq_F7QVcLUJb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/radlfabs/DLO-MNIST-OLD-SCHOOL/e/DLOM-47\n",
            "Starting training...\n",
            "epoch: 0  train_acc: 85.94%  test_acc: 92.62%  took 5.6s\n",
            "epoch: 1  train_acc: 93.86%  test_acc: 94.24%  took 5.2s\n",
            "epoch: 2  train_acc: 95.13%  test_acc: 95.07%  took 5.0s\n",
            "epoch: 3  train_acc: 95.79%  test_acc: 95.52%  took 5.1s\n",
            "epoch: 4  train_acc: 96.30%  test_acc: 96.01%  took 7.2s\n",
            "epoch: 5  train_acc: 96.66%  test_acc: 96.33%  took 6.0s\n",
            "epoch: 6  train_acc: 96.95%  test_acc: 96.30%  took 5.5s\n",
            "epoch: 7  train_acc: 97.23%  test_acc: 96.63%  took 5.8s\n",
            "epoch: 8  train_acc: 97.32%  test_acc: 96.30%  took 5.8s\n",
            "epoch: 9  train_acc: 97.47%  test_acc: 96.56%  took 7.9s\n",
            "epoch: 10  train_acc: 97.50%  test_acc: 96.68%  took 6.4s\n",
            "epoch: 11  train_acc: 97.63%  test_acc: 96.60%  took 5.2s\n",
            "epoch: 12  train_acc: 97.75%  test_acc: 96.68%  took 6.6s\n",
            "epoch: 13  train_acc: 97.79%  test_acc: 96.58%  took 6.8s\n",
            "epoch: 14  train_acc: 97.81%  test_acc: 96.72%  took 6.3s\n",
            "epoch: 15  train_acc: 97.88%  test_acc: 96.75%  took 6.3s\n",
            "epoch: 16  train_acc: 97.91%  test_acc: 96.55%  took 6.0s\n",
            "epoch: 17  train_acc: 97.94%  test_acc: 97.10%  took 6.6s\n",
            "epoch: 18  train_acc: 97.99%  test_acc: 96.96%  took 6.4s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m ix, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39miter\u001b[39m(train_dl)):\n\u001b[0;32m     95\u001b[0m   x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> 96\u001b[0m   batch_loss \u001b[39m=\u001b[39m train_batch(x, y, model, optimizer, loss_fn)\n\u001b[0;32m     97\u001b[0m   train_epoch_losses\u001b[39m.\u001b[39mappend(batch_loss)\n\u001b[0;32m     98\u001b[0m   is_correct \u001b[39m=\u001b[39m accuracy(x, y, model)\n",
            "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mtrain_batch\u001b[1;34m(x, y, model, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m     25\u001b[0m prediction \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     26\u001b[0m batch_loss \u001b[39m=\u001b[39m loss_fn(prediction, y)\n\u001b[1;32m---> 27\u001b[0m batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def get_model():\n",
        "  \"\"\"return a model, loss_fn and optimizer and implement a weight decay\"\"\"\n",
        "  model = nn.Sequential(\n",
        "        # nn.Conv2d(1, 32, 3, 1),\n",
        "        # nn.ReLU(),\n",
        "        # # nn.BatchNorm2d(32),\n",
        "        # nn.Conv2d(32, 64, 3, 1),\n",
        "        # nn.ReLU(),\n",
        "        # # nn.BatchNorm2d(64),\n",
        "        # nn.MaxPool2d(2),\n",
        "        # nn.Dropout(0.25),\n",
        "        # nn.Flatten(),\n",
        "        # nn.Linear(9216, 128),\n",
        "        # nn.ReLU(),\n",
        "        # nn.Dropout(0.5),\n",
        "        # nn.Linear(128, 10),\n",
        "        # nn.LogSoftmax(dim=1)\n",
        "    nn.Linear(28 * 28, 30),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(30, 20),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(20, 10),\n",
        "    \n",
        "    # nn.Linear(28 * 28, 600),\n",
        "    # nn.Tanh(),\n",
        "    \n",
        "    # nn.Linear(600, 300),\n",
        "    # nn.Tanh(),\n",
        "  \n",
        "    # nn.Linear(300, 150),\n",
        "    # nn.Tanh(),\n",
        "    \n",
        "    # nn.Linear(150, 50),\n",
        "    # nn.Sigmoid(),\n",
        "    \n",
        "    # nn.Linear(50, 30),\n",
        "    # nn.Sigmoid(),\n",
        "    \n",
        "    # nn.Linear(30, 20),\n",
        "    # nn.Sigmoid(),\n",
        "    \n",
        "    # nn.Linear(20, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        "    ).to(device)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  # loss_fn = nn.NLLLoss()\n",
        "  optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "  # optimizer = SGD(model.parameters(), lr=1e-2)\n",
        "  return model, loss_fn, optimizer\n",
        "\n",
        "run = neptune.init_run(\n",
        "    project=\"radlfabs/DLO-MNIST-OLD-SCHOOL\",\n",
        "    api_token=api_token,\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "train_dl, test_dl = get_data(batch_size)\n",
        "model, loss_fn, optimizer = get_model()\n",
        "# scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "run[\"init_weights\"] = \"normal\"\n",
        "run[\"model\"] = model.__str__()\n",
        "run[\"optimizer\"] = optimizer.__str__()\n",
        "run[\"loss_function\"] = loss_fn.__str__()\n",
        "run[\"dataset\"] = \"MNIST\"\n",
        "run[\"batch_size\"] = batch_size\n",
        "run[\"lr_scheduler\"] = scheduler.__str__() if scheduler else None\n",
        "run[\"epochs\"] = epochs\n",
        "#----------------------------------------------\n",
        "# Training >>>\n",
        "#\n",
        "print('Starting training...')\n",
        "\n",
        "model.apply(init_weights)  # hier werden die initialen Gewichte des Netzes zufällig gesetzt\n",
        "\n",
        "# epochs = 150\n",
        "\n",
        "early_stop = False\n",
        "best_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "consecutive_epochs_criterion = 3\n",
        "epsilon = 1e-3\n",
        "\n",
        "arrPlotX = []\n",
        "train_losses, train_accuracies = [], []\n",
        "test_losses, test_accuracies = [], []\n",
        "for epoch in range(epochs):\n",
        "  timeBeginEpoch = timer()\n",
        "  train_epoch_losses, train_epoch_accuracies = [], []\n",
        "  \n",
        "  for ix, batch in enumerate(iter(train_dl)):\n",
        "    x, y = batch\n",
        "    batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "    train_epoch_losses.append(batch_loss)\n",
        "    is_correct = accuracy(x, y, model)\n",
        "    train_epoch_accuracies.extend(is_correct)\n",
        "\n",
        "  train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "  train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
        " \n",
        "  for ix, batch in enumerate(iter(test_dl)):\n",
        "    x, y = batch\n",
        "    val_is_correct = accuracy(x, y, model)\n",
        "    validation_loss = loss(x, y, model, loss_fn)\n",
        "    \n",
        "  val_epoch_accuracy = np.mean(val_is_correct)\n",
        "  arrPlotX.append(epoch)\n",
        "  train_losses.append(train_epoch_loss)\n",
        "  train_accuracies.append(train_epoch_accuracy)\n",
        "  test_losses.append(validation_loss)\n",
        "  test_accuracies.append(val_epoch_accuracy)\n",
        "  run[\"train/loss\"].log(train_epoch_loss)\n",
        "  run[\"train/accuracy\"].log(train_epoch_accuracy)\n",
        "  run[\"test/loss\"].log(validation_loss)\n",
        "  run[\"test/accuracy\"].log(val_epoch_accuracy)\n",
        "  timeEndEpoch = timer()\n",
        "  print( f\"epoch: {epoch}  train_acc: {100 * train_epoch_accuracy:.2f}%  test_acc: {100 * val_epoch_accuracy:.2f}%  took {timeEndEpoch-timeBeginEpoch:.1f}s\" )   \n",
        "  scheduler.step(validation_loss)\n",
        "  \n",
        "    # Check for early stopping\n",
        "  if validation_loss < best_loss - epsilon:\n",
        "      best_loss = validation_loss\n",
        "      epochs_no_improve = 0\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "      if epochs_no_improve == consecutive_epochs_criterion:\n",
        "          early_stop = True\n",
        "          print(\"Early stopping criterion met\")\n",
        "          break\n",
        "        \n",
        "    # Check if early stopping criterion is met\n",
        "  if early_stop:\n",
        "    break\n",
        "  \n",
        "if USING_COLAB:\n",
        "  torch.save(model.state_dict(), '/content/drive/My Drive/ColabNotebooks/results/nnMnist_exp01.pt')\n",
        "else:\n",
        "  torch.save(model.state_dict(), 'nnMnist_exp01.pt')\n",
        "  run[\"model_file\"].upload('nnMnist_exp01.pt')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(arrPlotX, train_accuracies, label='train accuracy')\n",
        "plt.plot(arrPlotX, test_accuracies,   label='test accuracy')\n",
        "plt.legend()\n",
        "run[\"plot/accuracy\"].upload(fig)\n",
        "plt.show()\n",
        "\n",
        "if USING_COLAB:\n",
        "  plt.savefig('/content/drive/My Drive/ColabNotebooks/results/accuracies_exp0.png')\n",
        "else:\n",
        "  plt.savefig('accuracies_exp0.png')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(arrPlotX, train_losses, label='train')\n",
        "plt.plot(arrPlotX, test_losses, label='test')\n",
        "plt.legend()\n",
        "run[\"plot/loss\"].upload(fig)\n",
        "plt.show()\n",
        "\n",
        "if USING_COLAB:\n",
        "  plt.savefig('/content/drive/My Drive/ColabNotebooks/results/losses_exp0.png')\n",
        "else:\n",
        "  plt.savefig('losses_exp0.png')\n",
        "  \n",
        "run.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.weight torch.Size([600, 784])\n",
            "0.bias torch.Size([600])\n",
            "2.weight torch.Size([300, 600])\n",
            "2.bias torch.Size([300])\n",
            "4.weight torch.Size([150, 300])\n",
            "4.bias torch.Size([150])\n",
            "6.weight torch.Size([50, 150])\n",
            "6.bias torch.Size([50])\n",
            "8.weight torch.Size([30, 50])\n",
            "8.bias torch.Size([30])\n",
            "10.weight torch.Size([20, 30])\n",
            "10.bias torch.Size([20])\n",
            "12.weight torch.Size([10, 20])\n",
            "12.bias torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "# discover the tensor shapes of the layers in the model\n",
        "for name, param in model.named_parameters():\n",
        "  print(name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=30, bias=True)\n",
              "  (1): Tanh()\n",
              "  (2): Linear(in_features=30, out_features=20, bias=True)\n",
              "  (3): Tanh()\n",
              "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
              "  (5): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the network\n",
        "torch.save(model.state_dict(), 'models/nnMnist97p.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
