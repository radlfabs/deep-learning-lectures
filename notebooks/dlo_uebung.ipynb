{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a5f8c0e3f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neptune\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = ImageFolder(root=\"Rsp2023Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3967\n",
      "Validation set size: 992\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "# Set the root directory where the image data is located\n",
    "root = 'Rsp2023Train'\n",
    "apply_data_augmentation = True\n",
    "classes = sorted(os.listdir(root))\n",
    "# Define the transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),   # convert to grayscale\n",
    "    transforms.Resize((100, 150)),  # Resize the images to a consistent size\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5], std=[0.225])  # Normalize the images\n",
    "])\n",
    "\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomResizedCrop(300),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation((-5, 5)),\n",
    "    transforms.Resize((20, 30)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.225])\n",
    "])\n",
    "\n",
    "# Create data loaders for training and validation sets with data augmentation\n",
    "transform = augmentation_transform if apply_data_augmentation else transform\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "dataset = ImageFolder(root=root, transform=augmentation_transform)\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Define the split ratio for training and validation\n",
    "train_ratio = 0.8  # 80% of the data for training, 20% for validation\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Print the sizes of the training and validation sets\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "\n",
    "# Define the batch size for training and validation\n",
    "batch_size = 64\n",
    "\n",
    "# get number of workers on current machine\n",
    "num_workers_available = torch.multiprocessing.cpu_count()\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)  #, num_workers=num_workers_available)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)  #, num_workers=num_workers_available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 20, 220, 220]        520\n",
      "├─ReLU: 1-2                              [-1, 20, 220, 220]        --\n",
      "├─MaxPool2d: 1-3                         [-1, 20, 110, 110]        --\n",
      "├─Conv2d: 1-4                            [-1, 50, 106, 106]        25,050\n",
      "├─ReLU: 1-5                              [-1, 50, 106, 106]        --\n",
      "├─MaxPool2d: 1-6                         [-1, 50, 53, 53]          --\n",
      "├─ReLU: 1-7                              [-1, 500]                 --\n",
      "├─Linear: 1-8                            [-1, 4]                   2,004\n",
      "├─LogSoftmax: 1-9                        [-1, 4]                   --\n",
      "==========================================================================================\n",
      "Total params: 27,574\n",
      "Trainable params: 27,574\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 305.10\n",
      "==========================================================================================\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 11.67\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 11.97\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 20, 220, 220]        520\n",
      "├─ReLU: 1-2                              [-1, 20, 220, 220]        --\n",
      "├─MaxPool2d: 1-3                         [-1, 20, 110, 110]        --\n",
      "├─Conv2d: 1-4                            [-1, 50, 106, 106]        25,050\n",
      "├─ReLU: 1-5                              [-1, 50, 106, 106]        --\n",
      "├─MaxPool2d: 1-6                         [-1, 50, 53, 53]          --\n",
      "├─ReLU: 1-7                              [-1, 500]                 --\n",
      "├─Linear: 1-8                            [-1, 4]                   2,004\n",
      "├─LogSoftmax: 1-9                        [-1, 4]                   --\n",
      "==========================================================================================\n",
      "Total params: 27,574\n",
      "Trainable params: 27,574\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 305.10\n",
      "==========================================================================================\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 11.67\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 11.97\n",
      "==========================================================================================\n",
      "LeNet(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=140450, out_features=500, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=500, out_features=4, bias=True)\n",
      "  (logSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        # call the parent constructor\n",
    "        super(LeNet, self).__init__()\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=20,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = nn.Linear(in_features=800, out_features=500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=classes)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Calculate the input size for the first fully connected layer\n",
    "        fc1_input_size = x.size(1)\n",
    "        self.fc1 = nn.Linear(in_features=fc1_input_size, out_features=500)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of the LeNet model\n",
    "model = LeNet(numChannels=1, classes=num_classes)\n",
    "\n",
    "# model = get_model(num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "print(summary(model, (1, 224, 224)))\n",
    "print(model)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion)\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/radlfabs/DLO-U8/e/DLOU-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 1.3778, Train Accuracy: 0.2755 - Val Loss: 1.3672, Val Accuracy: 0.2873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] - Train Loss: 1.3649, Train Accuracy: 0.2730 - Val Loss: 1.3513, Val Accuracy: 0.2843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m total_predictions \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[39m# Iterate over the training batches\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_loader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain Batch\u001b[39m\u001b[39m\"\u001b[39m, position\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     51\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     52\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\PIL\\Image.py:933\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    886\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    887\u001b[0m ):\n\u001b[0;32m    888\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    935\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    936\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    937\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rosen\\miniconda3\\envs\\DLO\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "# log the run\n",
    "run = neptune.init_run(\n",
    "    project='DLO-U8', tags=['train', 'pytorch', 'rsp2023'],\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4Y2M2OTQxMC02ODFlLTQ4MTMtYWM5Yy1jNWIyNThmZWIyZjAifQ==\",\n",
    ")\n",
    "\n",
    "run[\"dataloader_params\"] = {\n",
    "    \"train_ratio\": train_ratio,\n",
    "    \"train_size\": len(train_set),\n",
    "    \"val_size\": len(val_set),\n",
    "    \"num_classes\": num_classes,\n",
    "    \"classes\": pprint.pformat(classes),\n",
    "    \"augmentation\": apply_data_augmentation,\n",
    "}\n",
    "\n",
    "run[\"transforms\"] = pprint.pformat(transform.__dict__)\n",
    "\n",
    "run[\"parameters\"] = {\n",
    "    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"weight_decay\": optimizer.param_groups[0][\"weight_decay\"],\n",
    "    \"patience\": patience,\n",
    "}\n",
    "\n",
    "run[\"model_summary\"] = pprint.pformat(model, indent=4)\n",
    "run[\"criterion_summary\"] = pprint.pformat(criterion, indent=4)\n",
    "run[\"optimizer_summary\"] = pprint.pformat(optimizer, indent=4)\n",
    "run[\"data_transforms\"] = pprint.pformat(transform, indent=4)\n",
    "run[\"augmentation_transforms\"] = pprint.pformat(augmentation_transform, indent=4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Iterate over the training batches\n",
    "    for images, labels in tqdm.tqdm(train_loader, total=len(train_loader), desc=\"Train Batch\", position=1, leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Compute average training loss and accuracy\n",
    "    epoch_loss = running_loss / len(train_set)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    run[\"train/loss\"].log(epoch_loss)\n",
    "    run[\"train/accuracy\"].log(epoch_accuracy)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    # Disable gradient calculation to speed up the inference\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in tqdm.tqdm(val_loader, total=len(val_loader), desc=\"Val Batch\", position=2, leave=False):\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_labels).item() * val_images.size(0)\n",
    "            \n",
    "            # Compute validation accuracy\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total_predictions += val_labels.size(0)\n",
    "            val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "    \n",
    "    # Compute average validation loss and accuracy\n",
    "    val_loss /= len(val_set)\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions\n",
    "    \n",
    "    run[\"eval/loss\"].log(val_loss)\n",
    "    run[\"eval/accuracy\"].log(val_accuracy)\n",
    "    \n",
    "    # Print training and validation statistics for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if the validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if counter == patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Defining Labels and Predictions\n",
    "truelabels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "print(\"Getting predictions from test set...\")\n",
    "for data, target in val_loader:\n",
    "    for label in target.data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction) \n",
    "\n",
    "try:\n",
    "    # Plot the confusion matrix\n",
    "    cm = confusion_matrix(truelabels, predictions)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "    f = plt.figure(figsize = (7,7))\n",
    "    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "    plt.xlabel(\"Predicted Class\", fontsize = 20)\n",
    "    plt.ylabel(\"True Class\", fontsize = 20)\n",
    "    run[\"confusion_matrix\"].upload(f)\n",
    "except:\n",
    "    run[\"eval_confusion_matrix\"].log(\"Eval Confusion Matrix Error\")\n",
    "    pass\n",
    "\n",
    "run_id = run[\"sys/id\"].fetch()\n",
    "run[\"best_epoch\"] = best_epoch\n",
    "filename = f\"model_cnn_{run_id}.pt\"\n",
    "torch.save(model, filename)\n",
    "run[\"model_checkpoints/model\"].upload(filename)\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
